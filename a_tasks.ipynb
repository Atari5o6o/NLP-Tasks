{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT1AFNejR2yx",
        "outputId": "98aceacc-b915-49d1-eb51-3f3b491580c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#task 1\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('brown')\n",
        "nltk.download('nps_chat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYKRJvlMR2y0",
        "outputId": "90831e96-6936-473f-e66a-923a61fe2a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading inuagural: Package 'inuagural' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package webtext to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.book import *\n",
        "nltk.download('inuagural')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('inaugural')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGbdPknTR2y1",
        "outputId": "0244fc7c-891a-43e5-8243-89a6dcb47a8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['It',\n",
              " 'was',\n",
              " 'among',\n",
              " 'these',\n",
              " 'that',\n",
              " 'Hinkle',\n",
              " 'identified',\n",
              " 'a',\n",
              " 'photograph',\n",
              " 'of']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import brown\n",
        "brown.categories\n",
        "from nltk.corpus import gutenberg\n",
        "brown.words(categories='humor')[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8H6NcC9R2y1",
        "outputId": "3efbe896-a423-474f-bc3f-af8050218222"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1789-Washington.txt',\n",
              " '1793-Washington.txt',\n",
              " '1797-Adams.txt',\n",
              " '1801-Jefferson.txt',\n",
              " '1805-Jefferson.txt',\n",
              " '1809-Madison.txt',\n",
              " '1813-Madison.txt',\n",
              " '1817-Monroe.txt',\n",
              " '1821-Monroe.txt',\n",
              " '1825-Adams.txt',\n",
              " '1829-Jackson.txt',\n",
              " '1833-Jackson.txt',\n",
              " '1837-VanBuren.txt',\n",
              " '1841-Harrison.txt',\n",
              " '1845-Polk.txt',\n",
              " '1849-Taylor.txt',\n",
              " '1853-Pierce.txt',\n",
              " '1857-Buchanan.txt',\n",
              " '1861-Lincoln.txt',\n",
              " '1865-Lincoln.txt',\n",
              " '1869-Grant.txt',\n",
              " '1873-Grant.txt',\n",
              " '1877-Hayes.txt',\n",
              " '1881-Garfield.txt',\n",
              " '1885-Cleveland.txt',\n",
              " '1889-Harrison.txt',\n",
              " '1893-Cleveland.txt',\n",
              " '1897-McKinley.txt',\n",
              " '1901-McKinley.txt',\n",
              " '1905-Roosevelt.txt',\n",
              " '1909-Taft.txt',\n",
              " '1913-Wilson.txt',\n",
              " '1917-Wilson.txt',\n",
              " '1921-Harding.txt',\n",
              " '1925-Coolidge.txt',\n",
              " '1929-Hoover.txt',\n",
              " '1933-Roosevelt.txt',\n",
              " '1937-Roosevelt.txt',\n",
              " '1941-Roosevelt.txt',\n",
              " '1945-Roosevelt.txt',\n",
              " '1949-Truman.txt',\n",
              " '1953-Eisenhower.txt',\n",
              " '1957-Eisenhower.txt',\n",
              " '1961-Kennedy.txt',\n",
              " '1965-Johnson.txt',\n",
              " '1969-Nixon.txt',\n",
              " '1973-Nixon.txt',\n",
              " '1977-Carter.txt',\n",
              " '1981-Reagan.txt',\n",
              " '1985-Reagan.txt',\n",
              " '1989-Bush.txt',\n",
              " '1993-Clinton.txt',\n",
              " '1997-Clinton.txt',\n",
              " '2001-Bush.txt',\n",
              " '2005-Bush.txt',\n",
              " '2009-Obama.txt',\n",
              " '2013-Obama.txt',\n",
              " '2017-Trump.txt',\n",
              " '2021-Biden.txt']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import inaugural\n",
        "\n",
        "inaugural.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q25jd9RR2y2",
        "outputId": "12a1bc3c-b6cd-48b7-8680-7b8061cdf5b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Vice',\n",
              " 'President',\n",
              " 'Cheney',\n",
              " ',',\n",
              " 'Mr',\n",
              " '.',\n",
              " 'Chief',\n",
              " 'Justice',\n",
              " ',',\n",
              " 'President',\n",
              " 'Carter',\n",
              " ',',\n",
              " 'President',\n",
              " 'Bush',\n",
              " ',',\n",
              " 'President',\n",
              " 'Clinton',\n",
              " ',',\n",
              " 'members',\n",
              " 'of',\n",
              " 'the',\n",
              " 'United',\n",
              " 'States',\n",
              " 'Congress',\n",
              " ',',\n",
              " 'reverend',\n",
              " 'clergy',\n",
              " ',',\n",
              " 'distinguished',\n",
              " 'guests',\n",
              " ',',\n",
              " 'fellow',\n",
              " 'citizens',\n",
              " ':',\n",
              " 'On',\n",
              " 'this',\n",
              " 'day',\n",
              " ',',\n",
              " 'prescribed',\n",
              " 'by',\n",
              " 'law',\n",
              " 'and',\n",
              " 'marked',\n",
              " 'by',\n",
              " 'ceremony',\n",
              " ',',\n",
              " 'we',\n",
              " 'celebrate',\n",
              " 'the',\n",
              " 'durable',\n",
              " 'wisdom',\n",
              " 'of',\n",
              " 'our',\n",
              " 'Constitution',\n",
              " ',',\n",
              " 'and',\n",
              " 'recall',\n",
              " 'the',\n",
              " 'deep',\n",
              " 'commitments',\n",
              " 'that',\n",
              " 'unite',\n",
              " 'our',\n",
              " 'country',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'grateful',\n",
              " 'for',\n",
              " 'the',\n",
              " 'honor',\n",
              " 'of',\n",
              " 'this',\n",
              " 'hour',\n",
              " ',',\n",
              " 'mindful',\n",
              " 'of',\n",
              " 'the',\n",
              " 'consequential',\n",
              " 'times',\n",
              " 'in',\n",
              " 'which',\n",
              " 'we',\n",
              " 'live',\n",
              " ',',\n",
              " 'and',\n",
              " 'determined',\n",
              " 'to',\n",
              " 'fulfill',\n",
              " 'the',\n",
              " 'oath',\n",
              " 'that',\n",
              " 'I',\n",
              " 'have',\n",
              " 'sworn',\n",
              " 'and',\n",
              " 'you',\n",
              " 'have',\n",
              " 'witnessed',\n",
              " '.']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inaugural.words(fileids = '2005-Bush.txt')[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFoEOG_VR2y2",
        "outputId": "65564458-758e-4f3b-dab7-b5bd94e45804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'Quick': 1, 'brown': 1, 'fox': 1, 'jumps': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#task 2\n",
        "import nltk\n",
        "text1 = 'Quick brown fox jumps over the lazy dog' \n",
        "fd= nltk.FreqDist(text1.split())\n",
        "fd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_wO2rPeR2y2",
        "outputId": "3e22328c-66bc-48ee-fcba-8a46a0ebadb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'Quick': 1, 'brown': 1, 'jumps': 1})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.probability import ConditionalFreqDist\n",
        "cfd = ConditionalFreqDist((len(word), word) for word in text1.split())\n",
        "cfd[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BSsjXZBR2y3",
        "outputId": "03def55c-6a3f-4534-d3ed-93278298203d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache C:\\Users\\saumy\\AppData\\Local\\Temp\\jieba.cache\n",
            "Loading model cost 0.813 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHINESE     TEXT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jieba\n",
        "seg=jieba.cut(\"CHINESE TEXT\",cut_all=True)\n",
        "print(\" \".join(seg))\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hdMd8SbR2y3",
        "outputId": "82ac8153-2079-48a6-b5db-fa3ee6a396c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NLP', 'class', 'is', 'fun']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent=\"NLP class is fun\"\n",
        "words = nltk.word_tokenize(sent)\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxML7Uy9R2y4",
        "outputId": "54c7c457-3929-4265-f0e4-f28effcd73f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Last_letter': 'l'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#task 3\n",
        "def gender_features(word):\n",
        "    return {'Last_letter':word[-1]}\n",
        "\n",
        "gender_features(\"Winston\")\n",
        "gender_features('anmol')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNmMD4RYR2y4",
        "outputId": "8f3077c7-6b15-4769-f922-27e7a555de6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import names\n",
        "nltk.download('names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYraj_C1R2y4",
        "outputId": "105c0849-9687-49a3-e7bb-3dacb6bfe35b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Percival', 'male'),\n",
              " ('Juli', 'female'),\n",
              " ('Kandace', 'female'),\n",
              " ('Heathcliff', 'male'),\n",
              " ('Konrad', 'male'),\n",
              " ('Terese', 'female'),\n",
              " ('Lavinie', 'female'),\n",
              " ('Candi', 'female'),\n",
              " ('Emelyne', 'female'),\n",
              " ('Mattie', 'female'),\n",
              " ('Andre', 'male'),\n",
              " ('Hedi', 'female'),\n",
              " ('Vernon', 'male'),\n",
              " ('Jere', 'male'),\n",
              " ('Kikelia', 'female')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labeled_names = ([(name, 'male') for name in names.words('male.txt')]+[(name, 'female') for name in names.words('female.txt')])\n",
        "import random\n",
        "random.shuffle(labeled_names)\n",
        "labeled_names[:15]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rs-h8_aR2y5",
        "outputId": "a4b8af37-97e2-4413-bef3-543bda89e179"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[({'Last_letter': 'l'}, 'male'),\n",
              " ({'Last_letter': 'i'}, 'female'),\n",
              " ({'Last_letter': 'e'}, 'female'),\n",
              " ({'Last_letter': 'f'}, 'male'),\n",
              " ({'Last_letter': 'd'}, 'male'),\n",
              " ({'Last_letter': 'e'}, 'female'),\n",
              " ({'Last_letter': 'e'}, 'female'),\n",
              " ({'Last_letter': 'i'}, 'female'),\n",
              " ({'Last_letter': 'e'}, 'female'),\n",
              " ({'Last_letter': 'e'}, 'female'),\n",
              " ({'Last_letter': 'e'}, 'male'),\n",
              " ({'Last_letter': 'i'}, 'female'),\n",
              " ({'Last_letter': 'n'}, 'male'),\n",
              " ({'Last_letter': 'e'}, 'male'),\n",
              " ({'Last_letter': 'a'}, 'female')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "featuresets = [(gender_features(n),gender) for (n,gender) in labeled_names]\n",
        "featuresets[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRTRyKWkR2y5",
        "outputId": "be46369c-cc2b-46a9-fd89-9496487ac407"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7944"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(featuresets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOGoF7eFR2y5"
      },
      "outputs": [],
      "source": [
        "train_set = featuresets[500:]\n",
        "test_test = featuresets[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNbuqAWIR2y6",
        "outputId": "6b31be05-711f-4dba-d45d-febe0bccdb07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'male'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "classifier.classify(gender_features('David'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg8GwYXaR2y6",
        "outputId": "44f03ea3-e7a9-46c8-9d44-805c54dad7a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'female'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.classify(gender_features('Saumya'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chRL-OAqR2y6",
        "outputId": "19d81ee9-04fa-4d05-dba5-011b69f9dd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.754\n"
          ]
        }
      ],
      "source": [
        "print(nltk.classify.accuracy(classifier,test_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVjK2nDXR2y7",
        "outputId": "07bbb94e-4a22-438d-88ab-6f9deb4a95a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'party', 'was', 'soooo', 'fun', '#superfun']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "text='The party was soooo fun #superfun'\n",
        "twttkn=TweetTokenizer()\n",
        "twttkn.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68DawZzLR2y7"
      },
      "outputs": [],
      "source": [
        "#task 4\n",
        "\n",
        "from urllib import request\n",
        "url=\"https://www.gutenberg.org/cache/epub/1524/pg1524.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO1npcL9R2y8",
        "outputId": "9ba5e9ec-b715-4d56-fa6c-b0a412870436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Hamlet', ',', 'by', 'William', 'Shakespeare', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included', 'with', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.org', '.', 'If', 'you', 'are', 'not', 'located', 'in', 'the', 'United', 'States', ',', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'eBook', '.', 'Title', ':', 'Hamlet']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(raw)\n",
        "\n",
        "print(tokens[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egHFBPpsR2y8",
        "outputId": "fae8f3e4-903b-420f-b082-8e3f504ebe2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('happi', 'joyou', 'cacti')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "a=porter.stem('happiness')\n",
        "b=porter.stem('joyous')\n",
        "c=porter.stem('cacti')\n",
        "a,b,c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5_o_jq1R2y8",
        "outputId": "9e351e37-248e-4cc5-aae8-7dbc307fbe4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'walk'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import RegexpStemmer\n",
        "reg = RegexpStemmer('ing')\n",
        "reg.stem('walking')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RM_p3MqR2y9",
        "outputId": "ea0347e6-8c33-405c-8e85-542f95f64f22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'baguet'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "snow = SnowballStemmer('french')\n",
        "snow.stem('baguette')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxflvgDaR2y9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "text = \"HAMLET. Angels and ministers of grace defend us! Be thou a spirit of health or goblin damn’d, Bring with thee airs from heaven or blasts from hell, Be thy intents wicked or charitable, Thou com’st in such a questionable shape That I will speak to thee. I’ll call thee Hamlet, King, father, royal Dane. O, answer me! Let me not burst in ignorance; but tell Why thy canoniz’d bones, hearsed in death, Have burst their cerements; why the sepulchre, Wherein we saw thee quietly inurn’d, Hath op’d his ponderous and marble jaws To cast thee up again! What may this mean, That thou, dead corse, again in complete steel, Revisit’st thus the glimpses of the moon, Making night hideous, and we fools of nature So horridly to shake our disposition With thoughts beyond the reaches of our souls? Say, why is this? Wherefore? What should we do?\"\n",
        "res = \"\"\n",
        "for i in text:\n",
        "    a=lancaster.stem(i)\n",
        "    res+=a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1nk6ai0R2y-",
        "outputId": "2f890e77-6654-4c7c-a7e5-0a98de0be3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hamlet.', 'angel', 'and', 'min', 'of', 'grac', 'defend', 'us!', 'be', 'thou', 'a', 'spirit', 'of', 'heal', 'or', 'goblin', 'damn’d,', 'bring', 'with', 'the', 'air', 'from', 'heav', 'or', 'blast', 'from', 'hell,', 'be', 'thy', 'int', 'wick', 'or', 'charitable,', 'thou', 'com’st', 'in', 'such', 'a', 'quest', 'shap', 'that', 'i', 'wil', 'speak', 'to', 'thee.', 'i’ll', 'cal', 'the', 'hamlet,', 'king,', 'father,', 'roy', 'dane.', 'o,', 'answ', 'me!', 'let', 'me', 'not', 'burst', 'in', 'ignorance;', 'but', 'tel', 'why', 'thy', 'canoniz’d', 'bones,', 'hears', 'in', 'death,', 'hav', 'burst', 'their', 'cerements;', 'why', 'the', 'sepulchre,', 'wherein', 'we', 'saw', 'the', 'quiet', 'inurn’d,', 'hath', 'op’d', 'his', 'pond', 'and', 'marbl', 'jaw', 'to', 'cast', 'the', 'up', 'again!', 'what', 'may', 'thi']\n"
          ]
        }
      ],
      "source": [
        "stemmed = [lancaster.stem(token) for token in text.split(\" \")]\n",
        "print(stemmed[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa2S2XRqR2y-",
        "outputId": "5cb500ab-e57f-468a-a3aa-6324dce73aca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hamlet. angels and ministers of grace defend us! be thou a spirit of health or goblin damn’d, bring with thee airs from heaven or blasts from hell, be thy intents wicked or charitable, thou com’st in such a questionable shape that i will speak to thee. i’ll call thee hamlet, king, father, royal dane. o, answer me! let me not burst in ignorance; but tell why thy canoniz’d bones, hearsed in death, have burst their cerements; why the sepulchre, wherein we saw thee quietly inurn’d, hath op’d his ponderous and marble jaws to cast thee up again! what may this mean, that thou, dead corse, again in complete steel, revisit’st thus the glimpses of the moon, making night hideous, and we fools of nature'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res[:700]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsLzNU_PR2y-",
        "outputId": "4c89c88c-b44e-497d-b61b-92e08e880176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cactus\n",
            "be\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\saumy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize(\"cacti\"))\n",
        "print(lemma.lemmatize(\"am\", pos='v'))\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0NpwQkLnR2y_"
      },
      "outputs": [],
      "source": [
        "#task 5\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(binary = True)\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "W9yLGzxGR2y_",
        "outputId": "8834dff1-68f5-4cda-99de-8abd01647fac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "corpus = [\"NLP is awesome\", \"I want to start an NLP company\", \"Top unicorns use NLP\"]\n",
        "\n",
        "vect.fit(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0kgIIyHR2y_",
        "outputId": "e4f5658a-3e85-49ca-f9e5-25e5d78eafec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "an:0\n",
            "awesome:1\n",
            "company:2\n",
            "is:3\n",
            "nlp:4\n",
            "start:5\n",
            "to:6\n",
            "top:7\n",
            "unicorns:8\n",
            "use:9\n",
            "want:10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocab = vect.vocabulary_\n",
        "for key in sorted(vocab.keys()): \n",
        "    print(\"{}:{}\".format(key,vocab[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNiK5k-3R2y_",
        "outputId": "40899629-3d8c-40f6-b8a3-280d781d4c91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.57735027]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "vect.transform([\"This NLP task is awesome\"]).toarray()\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cossim = cosine_similarity(vect.transform([\"NLP is an elective\"]).toarray(),vect.transform([\"Careers using NLP are attractive now\"]).toarray())\n",
        "\n",
        "cossim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvXlN6OxR2zA",
        "outputId": "52d5cb7e-571e-49ef-d2d3-85b3dd9954e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 11)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf = TfidfVectorizer()\n",
        "\n",
        "\n",
        "X = tf.fit_transform(corpus)\n",
        "\n",
        "tf.get_feature_names_out()\n",
        "\n",
        "print(X.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6Re_7BJSDZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}